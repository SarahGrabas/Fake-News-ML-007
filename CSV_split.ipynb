{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandarallel import pandarallel\n",
    "from langdetect import detect, DetectorFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_11404\\2103519170.py:2: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"C:/Users/Danie/OneDrive/Skrivebord/FN_Directory/CSV/995,000_rows.csv\")\n"
     ]
    }
   ],
   "source": [
    "#Reading the cleaning data set, insert own path to file 995,000_rows.csv\n",
    "df = pd.read_csv(\"995,000_rows.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7994a8f44d49958cf201fdc783c0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=124375), Label(value='0 / 124375')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "#DetectorFactory.seed = 0\n",
    "\n",
    "# Initialiser pandarallel\n",
    "pandarallel.initialize(progress_bar=True)  #giver os en progress\n",
    "  # kan også manuelt indstilles\n",
    "\n",
    "def detect_language():\n",
    "  from langdetect import detect, DetectorFactory\n",
    "  DetectorFactory.seed = 0\n",
    "  df['content']=df['content'].astype(str).fillna(\"\")\n",
    "  df['language']=df['content'].parallel_apply(lambda x: detect(x))\n",
    "\n",
    "detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "en       977844\n",
       "ru        11797\n",
       "fr         1381\n",
       "es         1348\n",
       "el          879\n",
       "it          542\n",
       "de          476\n",
       "ar          359\n",
       "nl           84\n",
       "pt           70\n",
       "vi           65\n",
       "no           33\n",
       "da           24\n",
       "tl           19\n",
       "ca           15\n",
       "et           14\n",
       "af           11\n",
       "cy           10\n",
       "sv            7\n",
       "id            3\n",
       "zh-cn         3\n",
       "ro            2\n",
       "ko            2\n",
       "sk            2\n",
       "he            2\n",
       "pl            2\n",
       "bg            1\n",
       "hu            1\n",
       "hi            1\n",
       "so            1\n",
       "hr            1\n",
       "lt            1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['language'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en = df[df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "en    977844\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "reliable                      218564\n",
       "political                     194518\n",
       "bias                          133232\n",
       "fake                          104883\n",
       "conspiracy                     97314\n",
       "rumor                          56445\n",
       "unknown                        43534\n",
       "unreliable                     35332\n",
       "clickbait                      27412\n",
       "junksci                        14040\n",
       "satire                         13160\n",
       "hate                            8779\n",
       "2018-02-10 13:43:39.521661         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of non-English articles in the corpus: 1.7544720834816188\n"
     ]
    }
   ],
   "source": [
    "#Calcuate the percentage of non-english articles in the total corpus\n",
    "count_non_english = len(df[df['language'] != 'en'])\n",
    "count_english = len(df[df['language'] == 'en'])\n",
    "percent = (count_non_english / count_english)*100\n",
    "print(\"Percentage of non-English articles in the corpus:\", percent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giving every type a broad type, so all articles are marked as either Fake or Reliable news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning every type a new category, so there only are two types 'Fake News' or 'Reliable News'\n",
    "category_mapping = {\n",
    "    \"fake\": \"Fake News\",\n",
    "    \"satire\": \"Fake News\",\n",
    "    \"bias\": \"Fake News\",\n",
    "    \"conspiracy\": \"Fake News\",\n",
    "    \"junksci\": \"Fake News\",\n",
    "    \"hate\": \"Reliable News\",\n",
    "    \"clickbait\": \"Reliable News\",\n",
    "    \"unreliable\": \"Fake News\",\n",
    "    \"political\": \"Reliable News\",\n",
    "    \"reliable\": \"Reliable News\",  \n",
    "    \"rumor\": \"Fake News\"\n",
    "}\n",
    "\n",
    "#Appling the mapping to the 'type' column, making a new column called 'broad_category'\n",
    "df['broad_category'] = df['type'].map(category_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing rows with NaN-values in 'broad_category'\n",
    "df = df.dropna(subset=['broad_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Article with no broad category is removed\n",
    "df = df[~df[\"broad_category\"].isin(['unknown']) & df[\"broad_category\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "broad_category\n",
       "Fake News        454406\n",
       "Reliable News    449273\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['broad_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 722943 rows\n",
      "Validation: 90368 rows\n",
      "Test: 90368 rows\n"
     ]
    }
   ],
   "source": [
    "#Splitting data into training set (80%) and the rest (20%) \n",
    "train, temp = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "#Splitting the rest into validation set(50% of 20% = 10%) and testset (50% of 20% = 10%)\n",
    "val, test = train_test_split(temp, test_size=0.5, random_state=0)\n",
    "\n",
    "#Size of the three sets\n",
    "print(f\"Training data: {len(train)} rows\")\n",
    "print(f\"Validation: {len(val)} rows\")\n",
    "print(f\"Test: {len(test)} rows\")\n",
    "\n",
    "\n",
    "#Converting the sets ad csv-files\n",
    "train.to_csv('full_train_set.csv', index=False)\n",
    "val.to_csv('full_val_set.csv', index=False)\n",
    "test.to_csv('full_test_set.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
